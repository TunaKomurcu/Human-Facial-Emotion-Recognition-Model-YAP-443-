{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75835c0-355b-4d66-a899-36ee14d52ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Gerekli KÃ¼tÃ¼phaneler ve Ayarlar\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Cihaz ayarÄ± (GPU varsa GPU, yoksa CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64bfde-f2b2-4c3c-904a-17a8f101998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        out = self.sigmoid(avg_out + max_out)\n",
    "        return x * out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d632b-bb67-4640-971c-559a4f0ca8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        return F.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee2dab-ccbd-4fcd-aa7c-bfcd0dd27a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(HybridEmotionCNN, self).__init__()\n",
    "        \n",
    "        # --- Convolution + BatchNorm + CBAM ---\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)  # **Tek kanal giriÅŸ**\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.cbam1 = CBAMBlock(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.cbam2 = CBAMBlock(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.cbam3 = CBAMBlock(256)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.cbam4 = CBAMBlock(512)\n",
    "\n",
    "        # --- Residual Blocks ---\n",
    "        self.residual1 = ResidualBlock(512)\n",
    "        self.residual2 = ResidualBlock(512)\n",
    "        self.residual3 = ResidualBlock(512)\n",
    "\n",
    "        # --- Fully Connected Katmanlar ---\n",
    "        self.fc1 = nn.Linear(512 * 4 * 4, 2048)  # **DÃ¼zleÅŸtirme iÅŸlemi iÃ§in uygun boyut**\n",
    "        self.bn6 = nn.LayerNorm(2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, num_classes)  # 6 sÄ±nÄ±f Ã§Ä±ktÄ±sÄ±\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Convolutional Bloklar ---\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.cbam1(x)\n",
    "\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.cbam2(x)\n",
    "\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.cbam3(x)\n",
    "\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.cbam4(x)\n",
    "\n",
    "        # --- Residual Bloklar ---\n",
    "        x = self.residual1(x)\n",
    "        x = self.residual2(x)\n",
    "        x = self.residual3(x)\n",
    "\n",
    "        # **HATA DÃœZELTÄ°LDÄ°: Flatten iÅŸlemi eksikti**\n",
    "        x = torch.flatten(x, 1)  # Fully Connected katmanlar iÃ§in dÃ¼zleÅŸtirme\n",
    "\n",
    "        # --- Fully Connected Katmanlar ---\n",
    "        x = F.relu(self.bn6(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)  # SonuÃ§: 6 sÄ±nÄ±fÄ±n olasÄ±lÄ±klarÄ±\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a3fe8-1d90-4aa8-b7ff-27632804491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AynÄ± model yapÄ±sÄ±nÄ± oluÅŸturun ve cihazÄ± ayarlayÄ±n\n",
    "model = HybridEmotionCNN(num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321f524-7b87-412f-9186-3dbce4d0064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer ve Loss fonksiyonu tanÄ±mlanmasÄ±\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a4cc0-bf5a-4222-b6da-62715bb790e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# KÃ¶k klasÃ¶r\n",
    "base_dir = \"C:/Users/Tuna/Downloads/images\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# SÄ±nÄ±flar (Ã¶rneÄŸin: happy, sad, etc.)\n",
    "classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "\n",
    "# Test klasÃ¶rÃ¼nÃ¼ oluÅŸtur\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"\\nâ–¶ï¸ {cls} sÄ±nÄ±fÄ± iÃ§in Ã¶rnekler hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "    # TÃ¼m resimleri topla\n",
    "    train_cls_path = os.path.join(train_dir, cls)\n",
    "    val_cls_path = os.path.join(val_dir, cls)\n",
    "\n",
    "    all_images = []\n",
    "\n",
    "    for folder in [train_cls_path, val_cls_path]:\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        all_images += [(os.path.join(folder, f), f) for f in files]\n",
    "\n",
    "    # KarÄ±ÅŸtÄ±r\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    total_count = len(all_images)\n",
    "    test_count = int(0.2 * total_count)\n",
    "\n",
    "    print(f\"Toplam: {total_count} -> Test'e ayrÄ±lacak: {test_count} gÃ¶rsel\")\n",
    "\n",
    "    # Test iÃ§in seÃ§ilen gÃ¶rselleri yeni klasÃ¶re kopyala\n",
    "    selected_test_images = all_images[:test_count]\n",
    "    for src_path, filename in selected_test_images:\n",
    "        dst_path = os.path.join(test_dir, cls, filename)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"âœ… {cls}: Test seti tamamlandÄ±. ({len(selected_test_images)} resim)\")\n",
    "\n",
    "print(\"\\nğŸ‰ TÃ¼m sÄ±nÄ±flar iÃ§in test seti baÅŸarÄ±yla oluÅŸturuldu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31214f-261f-4a1a-ae5a-dd0bdeb46d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: CNN Modeli TanÄ±mlamasÄ± (DeepCNNFeatureExtractor)\n",
    "# Bu model, 1 kanallÄ± (grayscale) 64x64 gÃ¶rÃ¼ntÃ¼ler iÃ§in sÄ±fÄ±rdan eÄŸitilecek ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.\n",
    "class DeepCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNNFeatureExtractor, self).__init__()\n",
    "        # Blok 1: 1 -> 32 kanal\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 64x64 -> 32x32\n",
    "\n",
    "        # Blok 2: 32 -> 64 kanal\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
    "\n",
    "        # Blok 3: 64 -> 128 kanal\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5   = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6   = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 16x16 -> 8x8\n",
    "\n",
    "        # Tam baÄŸlantÄ±lÄ± katman: Ã–zellik vektÃ¶rÃ¼ boyutu 128*8*8 = 8192'den 256'ya indirme\n",
    "        self.fc = nn.Linear(128 * 8 * 8, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.fc(x)  # 256 boyutlu Ã¶zellik vektÃ¶rÃ¼\n",
    "        return features\n",
    "\n",
    "# Ã–rnek test (isteÄŸe baÄŸlÄ±)\n",
    "if __name__ == '__main__':\n",
    "    dummy = torch.randn(1, 1, 64, 64).to(device)\n",
    "    feat = DeepCNNFeatureExtractor().to(device)(dummy)\n",
    "    print(\"CNN Ã¶zellik vektÃ¶rÃ¼ boyutu:\", feat.shape)  # Beklenen: [1, 256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c5718-b101-4533-b69f-414583aa82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: HOG Ã–zellik Ã‡Ä±karÄ±m Fonksiyonu\n",
    "def extract_hog_features(image, pixels_per_cell=(8,8), cells_per_block=(2,2), orientations=9):\n",
    "    \"\"\"\n",
    "    image: GiriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼ (grayscale, numpy array, boyut: 64x64)\n",
    "    HOG Ã¶zellikleri Ã§Ä±karÄ±lÄ±r.\n",
    "    \"\"\"\n",
    "    # image'Ä± float formatÄ±na Ã§eviriyoruz\n",
    "    image = np.float32(image) / 255.0\n",
    "    features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block, block_norm='L2-Hys', feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Ã–rnek test\n",
    "test_image = np.random.randint(0, 256, (64, 64), dtype=np.uint8)\n",
    "hog_feat = extract_hog_features(test_image)\n",
    "print(\"HOG Ã¶zelliÄŸi boyutu:\", hog_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524764f-db33-4014-9e11-ce3773211652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: LPG Ã–zellik Ã‡Ä±karÄ±m Fonksiyonu (Sobel GradyanlarÄ± ile)\n",
    "def extract_lpg_features(image):\n",
    "    \"\"\"\n",
    "    image: GiriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼ (grayscale, numpy array, boyut: 64x64)\n",
    "    Sobel gradyanlarÄ± kullanÄ±larak yÃ¶n histogramÄ± hesaplanÄ±r.\n",
    "    \"\"\"\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    angle = cv2.phase(grad_x, grad_y, angleInDegrees=True)\n",
    "    hist = cv2.calcHist([np.uint8(angle)], [0], None, [36], [0, 360])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# Ã–rnek test\n",
    "lpg_feat = extract_lpg_features(test_image)\n",
    "print(\"LPG Ã¶zelliÄŸi boyutu:\", lpg_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a716b5-5f8f-4a76-b8c6-760a827cbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Hibrit Ã–zellik Ã‡Ä±karÄ±m Fonksiyonu (CNN, HOG, LPG)\n",
    "def extract_hybrid_features(image_tensor, cnn_model, original_image):\n",
    "    \"\"\"\n",
    "    image_tensor: CNN iÃ§in Ã¶n iÅŸlenmiÅŸ tensÃ¶r (1x64x64)\n",
    "    original_image: Orijinal veya yeniden boyutlandÄ±rÄ±lmÄ±ÅŸ grayscale gÃ¶rÃ¼ntÃ¼ (64x64 numpy array)\n",
    "    \"\"\"\n",
    "    # CNN Ã¶zellikleri\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat = cnn_model(image_tensor.unsqueeze(0).to(device))\n",
    "        cnn_feat = feat.squeeze(0).cpu().numpy()  # Boyut: [256]\n",
    "\n",
    "    hog_feat = extract_hog_features(original_image)  # Boyut: Ã¶rneÄŸin ~1764\n",
    "    lpg_feat = extract_lpg_features(original_image)  # Boyut: [36]\n",
    "\n",
    "    # Hibrit Ã¶zellik vektÃ¶rÃ¼: CNN + HOG + LPG\n",
    "    hybrid_feat = np.concatenate([cnn_feat, hog_feat, lpg_feat])\n",
    "    return hybrid_feat\n",
    "\n",
    "# Ã–rnek test\n",
    "dummy_tensor = torch.randn(1, 64, 64)  # rastgele Ã¶rnek (1x64x64)\n",
    "hybrid_test = extract_hybrid_features(dummy_tensor, DeepCNNFeatureExtractor().to(device), test_image)\n",
    "print(\"Hibrit Ã¶zellik vektÃ¶rÃ¼ boyutu:\", hybrid_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fca54d-e4b2-4834-b9c5-7f78369c11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Dataset SÄ±nÄ±fÄ± ve Transform AyarlarÄ±\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class EmotionDatasetFromSamples(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        samples: [(img_path, label)] formatÄ±nda Ã¶rneklerin listesi\n",
    "        transform: Resim iÃ§in uygulanacak dÃ¶nÃ¼ÅŸÃ¼mler\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        # Orijinal gÃ¶rÃ¼ntÃ¼yÃ¼ cv2 ile oku ve yeniden boyutlandÄ±r\n",
    "        original_image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original_image is None:\n",
    "            raise ValueError(f\"Resim okunamadÄ±: {img_path}\")\n",
    "        original_image = cv2.resize(original_image, (64, 64))\n",
    "        # PIL ile aÃ§Ä±p grayscale'e Ã§evir\n",
    "        image_pil = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)\n",
    "        else:\n",
    "            from torchvision.transforms import ToTensor\n",
    "            image_tensor = ToTensor()(image_pil)\n",
    "        return image_tensor, original_image, label\n",
    "        \n",
    "# Transform: Grayscale, Resize (64x64), ToTensor, Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41330e25-6411-4567-b6af-dbc10b1263bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Hibrit Ã–zellik Ã‡Ä±karÄ±m Fonksiyonu ve Veri YollarÄ±nÄ±n AyarlanmasÄ±\n",
    "def extract_hybrid_features_from_dataset(dataset, cnn_model):\n",
    "    features_list, labels_list = [], []\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (img_tensor, original_image, label) in enumerate(dataset):\n",
    "            feat = extract_hybrid_features(img_tensor, cnn_model, original_image)\n",
    "            features_list.append(feat)\n",
    "            labels_list.append(label)\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"{idx} Ã¶rnek iÅŸlendi...\")\n",
    "    return np.array(features_list), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998eac02-0f23-4b0d-8d54-172cbfca4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Mevcut klasÃ¶rler\n",
    "base_dir = \"C:/Users/Tuna/Downloads/images\"\n",
    "existing_splits = [\"train\", \"validation\", \"test\"]\n",
    "temp_pool_dir = \"C:/Users/Tuna/Downloads/images_all_temp\"  # geÃ§ici havuz\n",
    "\n",
    "# GeÃ§ici havuz klasÃ¶rÃ¼nÃ¼ oluÅŸtur\n",
    "os.makedirs(temp_pool_dir, exist_ok=True)\n",
    "\n",
    "# TÃ¼m verileri geÃ§ici havuza kopyala (var olan klasÃ¶r yapÄ±sÄ± korunmadan)\n",
    "for split in existing_splits:\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "    classes = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(split_dir, cls)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # GeÃ§ici havuzda her sÄ±nÄ±f iÃ§in klasÃ¶r oluÅŸtur\n",
    "        temp_class_dir = os.path.join(temp_pool_dir, cls)\n",
    "        os.makedirs(temp_class_dir, exist_ok=True)\n",
    "        for img in images:\n",
    "            src = os.path.join(class_path, img)\n",
    "            dst = os.path.join(temp_class_dir, img)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(\"TÃ¼m veriler geÃ§ici havuza kopyalandÄ±:\", temp_pool_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cde917-c507-491d-b9bc-d087b1f33444",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"C:\\\\Users\\\\Tuna\\\\Downloads\\\\images_all_temp\"  \n",
    "all_samples = []\n",
    "\n",
    "classes = sorted(os.listdir(train_folder))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classes = sorted(os.listdir(train_folder))\n",
    "\n",
    "for label, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(train_folder, class_name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            all_samples.append((img_path, label))\n",
    "        else:\n",
    "            print(\"Eksik dosya:\", img_path)\n",
    "            \n",
    "# EÄŸitim ve test verilerini ayÄ±r (60% eÄŸitim, 40% test)\n",
    "train_samples, test_samples = train_test_split(\n",
    "    all_samples, test_size=0.4, stratify=[label for _, label in all_samples], random_state=42\n",
    ")\n",
    "\n",
    "# DoÄŸrulama ve test verilerini ayÄ±r (20% + 20%)\n",
    "val_samples, test_samples = train_test_split(\n",
    "    test_samples, test_size=0.5, stratify=[label for _, label in test_samples], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Yeni datasetleri oluÅŸtur\n",
    "train_dataset = EmotionDatasetFromSamples(train_samples, transform=transform)\n",
    "val_dataset = EmotionDatasetFromSamples(val_samples, transform=transform)\n",
    "test_dataset = EmotionDatasetFromSamples(test_samples, transform=transform)\n",
    "\n",
    "\n",
    "# Dataloaders oluÅŸtur\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(\"SÄ±nÄ±flar:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba9cd5-f14e-4625-a6e3-ef4050292c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik Ã‡Ä±karÄ±mÄ±\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Parametre aralÄ±ÄŸÄ±nÄ± daraltÄ±lmÄ±ÅŸ ve n_iter belirlenmiÅŸ parametre daÄŸÄ±lÄ±mÄ±\n",
    "param_distributions = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# SVM iÃ§in temel fonksiyon\n",
    "def base_svm(**kwargs):\n",
    "    kwargs.setdefault('class_weight', 'balanced')\n",
    "    return SVC(kernel=\"rbf\", **kwargs)\n",
    "\n",
    "# Hibrit Ã¶zellik Ã§Ä±karÄ±mÄ±\n",
    "print(\"Hibrit Ã¶znitelikler Ã§Ä±karÄ±lÄ±yor (CNN + HOG + LPG)...\")\n",
    "\n",
    "# Burada, tÃ¼m verinin yeniden bÃ¶lÃ¼nmÃ¼ÅŸ datasetlerini (train, validation, test) kullanÄ±yoruz.\n",
    "# Bu datasetlerin, tÃ¼m veriyi bir havuzda toplayÄ±p 60/20/20 oranÄ±nda bÃ¶ldÃ¼ÄŸÃ¼nÃ¼z bÃ¶lÃ¼mde oluÅŸturduÄŸunuzu varsayÄ±yoruz.\n",
    "X_train, y_train = extract_hybrid_features_from_dataset(train_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "X_val, y_val     = extract_hybrid_features_from_dataset(val_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "X_test, y_test   = extract_hybrid_features_from_dataset(test_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "\n",
    "print(\"Ã–znitelik Ã§Ä±karÄ±mÄ± tamamlandÄ±.\")\n",
    "print(\"Train veri boyutu: \", X_train.shape)\n",
    "print(\"Validation veri boyutu: \", X_val.shape)\n",
    "print(\"Test veri boyutu: \", X_test.shape)\n",
    "\n",
    "# PCA ile boyut indirgeme: Ã–rneÄŸin, orijinal Ã¶zellik boyutunu 500'e indiriyoruz\n",
    "pca = PCA(n_components=500, random_state=42)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_val_reduced   = pca.transform(X_val)\n",
    "X_test_reduced  = pca.transform(X_test)\n",
    "\n",
    "print(\"PCA sonrasÄ± boyutlar:\")\n",
    "print(\"Train: \", X_train_reduced.shape)\n",
    "print(\"Validation: \", X_val_reduced.shape)\n",
    "print(\"Test: \", X_test_reduced.shape)\n",
    "\n",
    "# PCA nesnesini kaydedelim\n",
    "joblib.dump(pca, \"pca_transformer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e07865-973f-4ede-96ce-7e0d04cf1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint kontrolÃ¼ ve yÃ¼klemesi\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if \"epoch\" in checkpoint:\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        print(f\"âœ… EÄŸitime {start_epoch}. epoch'tan devam ediliyor.\", flush=True)\n",
    "    else:\n",
    "        print(\"âš ï¸ Checkpoint dosyasÄ±nda 'epoch' bilgisi bulunamadÄ±! EÄŸitim 0'dan baÅŸlatÄ±lÄ±yor.\", flush=True)\n",
    "        start_epoch = 0\n",
    "else:\n",
    "    print(\"âš ï¸ Checkpoint bulunamadÄ±! EÄŸitim 0'dan baÅŸlatÄ±lÄ±yor.\", flush=True)\n",
    "    start_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2935d5-0bed-4633-8658-00ad1e394fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"cnn_model_weights.pth\", map_location=device, weights_only=True)\n",
    "print(checkpoint.keys())  # Checkpoint iÃ§indeki tÃ¼m anahtarlarÄ± yazdÄ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fc23b-b487-40d2-9880-e4de593a335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Alternatif checkpoint kontrolÃ¼ (weights dosyasÄ±)\n",
    "checkpoint_weights_path = \"cnn_model_weights.pth\"\n",
    "if os.path.exists(checkpoint_weights_path):\n",
    "    checkpoint = torch.load(checkpoint_weights_path, map_location=device)\n",
    "    print(\"Checkpoint anahtarlarÄ±:\", checkpoint.keys(), flush=True)\n",
    "    if \"epoch\" in checkpoint:\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"âœ… EÄŸitime {start_epoch}. epoch'tan devam ediliyor.\", flush=True)\n",
    "    else:\n",
    "        print(\"âš ï¸ Checkpoint dosyasÄ±nda 'epoch' bilgisi bulunamadÄ±! EÄŸitim 0'dan baÅŸlatÄ±lÄ±yor.\", flush=True)\n",
    "        start_epoch = 0\n",
    "else:\n",
    "    print(\"âš ï¸ Alternatif checkpoint bulunamadÄ±! EÄŸitim 0'dan baÅŸlatÄ±lÄ±yor.\", flush=True)\n",
    "    start_epoch = 0\n",
    "\n",
    "epochs = 10  # KaÃ§ epoch devam edeceÄŸini belirle\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # tqdm ile batch ilerlemesini gÃ¶steriyoruz\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100)\n",
    "    for images, _, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Ortalama Loss: {avg_loss:.4f}\", flush=True)\n",
    "\n",
    "    # Her epoch sonunda aÄŸÄ±rlÄ±klarÄ± ve eÄŸitimin ilerleme durumunu kaydet\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict()\n",
    "    }, \"checkpoint2.pth\")\n",
    "\n",
    "print(\"âœ… EÄŸitim tamamlandÄ± ve model kaydedildi!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb497c-b03c-49a5-b9e6-bbcdf62409a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model aÄŸÄ±rlÄ±klarÄ±nÄ± .pkl uzantÄ±lÄ± dosyaya kaydet\n",
    "with open(\"cnn_model_weights.pkl\", \"wb\") as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "\n",
    "print(\"âœ… Model aÄŸÄ±rlÄ±klarÄ± cnn_model_weights.pkl dosyasÄ±na kaydedildi!\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
