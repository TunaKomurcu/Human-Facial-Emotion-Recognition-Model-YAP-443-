{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caf21b-5624-4ee7-ae0a-ad18ae53ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Gerekli K√ºt√ºphaneler ve Ayarlar\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Cihaz ayarƒ± (GPU varsa GPU, yoksa CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431d3f8-d82a-439d-a782-bf665d01cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# K√∂k klas√∂r\n",
    "base_dir = \"C:/Users/Tuna/Downloads/images\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Sƒ±nƒ±flar (√∂rneƒüin: happy, sad, etc.)\n",
    "classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "\n",
    "# Test klas√∂r√ºn√º olu≈ütur\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"\\n‚ñ∂Ô∏è {cls} sƒ±nƒ±fƒ± i√ßin √∂rnekler hazƒ±rlanƒ±yor...\")\n",
    "\n",
    "    # T√ºm resimleri topla\n",
    "    train_cls_path = os.path.join(train_dir, cls)\n",
    "    val_cls_path = os.path.join(val_dir, cls)\n",
    "\n",
    "    all_images = []\n",
    "\n",
    "    for folder in [train_cls_path, val_cls_path]:\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        all_images += [(os.path.join(folder, f), f) for f in files]\n",
    "\n",
    "    # Karƒ±≈ütƒ±r\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    total_count = len(all_images)\n",
    "    test_count = int(0.2 * total_count)\n",
    "\n",
    "    print(f\"Toplam: {total_count} -> Test'e ayrƒ±lacak: {test_count} g√∂rsel\")\n",
    "\n",
    "    # Test i√ßin se√ßilen g√∂rselleri yeni klas√∂re kopyala\n",
    "    selected_test_images = all_images[:test_count]\n",
    "    for src_path, filename in selected_test_images:\n",
    "        dst_path = os.path.join(test_dir, cls, filename)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"‚úÖ {cls}: Test seti tamamlandƒ±. ({len(selected_test_images)} resim)\")\n",
    "\n",
    "print(\"\\nüéâ T√ºm sƒ±nƒ±flar i√ßin test seti ba≈üarƒ±yla olu≈üturuldu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706eda0-7c6f-46c1-9dee-8a8f8e6b41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: CNN Modeli Tanƒ±mlamasƒ± (DeepCNNFeatureExtractor)\n",
    "# Bu model, 1 kanallƒ± (grayscale) 64x64 g√∂r√ºnt√ºler i√ßin sƒ±fƒ±rdan eƒüitilecek ≈üekilde tasarlanmƒ±≈ütƒ±r.\n",
    "class DeepCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNNFeatureExtractor, self).__init__()\n",
    "        # Blok 1: 1 -> 32 kanal\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 64x64 -> 32x32\n",
    "\n",
    "        # Blok 2: 32 -> 64 kanal\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
    "\n",
    "        # Blok 3: 64 -> 128 kanal\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5   = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6   = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 16x16 -> 8x8\n",
    "\n",
    "        # Tam baƒülantƒ±lƒ± katman: √ñzellik vekt√∂r√º boyutu 128*8*8 = 8192'den 256'ya indirme\n",
    "        self.fc = nn.Linear(128 * 8 * 8, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.fc(x)  # 256 boyutlu √∂zellik vekt√∂r√º\n",
    "        return features\n",
    "\n",
    "# √ñrnek test (isteƒüe baƒülƒ±)\n",
    "if __name__ == '__main__':\n",
    "    dummy = torch.randn(1, 1, 64, 64).to(device)\n",
    "    feat = DeepCNNFeatureExtractor().to(device)(dummy)\n",
    "    print(\"CNN √∂zellik vekt√∂r√º boyutu:\", feat.shape)  # Beklenen: [1, 256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2c52a-0b35-407c-a5e7-52ec2e5b9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: HOG √ñzellik √áƒ±karƒ±m Fonksiyonu\n",
    "def extract_hog_features(image, pixels_per_cell=(8,8), cells_per_block=(2,2), orientations=9):\n",
    "    \"\"\"\n",
    "    image: Giri≈ü g√∂r√ºnt√ºs√º (grayscale, numpy array, boyut: 64x64)\n",
    "    HOG √∂zellikleri √ßƒ±karƒ±lƒ±r.\n",
    "    \"\"\"\n",
    "    # image'ƒ± float formatƒ±na √ßeviriyoruz\n",
    "    image = np.float32(image) / 255.0\n",
    "    features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block, block_norm='L2-Hys', feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# √ñrnek test\n",
    "test_image = np.random.randint(0, 256, (64, 64), dtype=np.uint8)\n",
    "hog_feat = extract_hog_features(test_image)\n",
    "print(\"HOG √∂zelliƒüi boyutu:\", hog_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87699a30-87bd-4a93-8309-4d046c199a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: LPG √ñzellik √áƒ±karƒ±m Fonksiyonu (Sobel Gradyanlarƒ± ile)\n",
    "def extract_lpg_features(image):\n",
    "    \"\"\"\n",
    "    image: Giri≈ü g√∂r√ºnt√ºs√º (grayscale, numpy array, boyut: 64x64)\n",
    "    Sobel gradyanlarƒ± kullanƒ±larak y√∂n histogramƒ± hesaplanƒ±r.\n",
    "    \"\"\"\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    angle = cv2.phase(grad_x, grad_y, angleInDegrees=True)\n",
    "    hist = cv2.calcHist([np.uint8(angle)], [0], None, [36], [0, 360])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# √ñrnek test\n",
    "lpg_feat = extract_lpg_features(test_image)\n",
    "print(\"LPG √∂zelliƒüi boyutu:\", lpg_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda70f5c-7fd9-4b5d-bfb4-63a906d40270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Hibrit √ñzellik √áƒ±karƒ±m Fonksiyonu (CNN, HOG, LPG)\n",
    "def extract_hybrid_features(image_tensor, cnn_model, original_image):\n",
    "    \"\"\"\n",
    "    image_tensor: CNN i√ßin √∂n i≈ülenmi≈ü tens√∂r (1x64x64)\n",
    "    original_image: Orijinal veya yeniden boyutlandƒ±rƒ±lmƒ±≈ü grayscale g√∂r√ºnt√º (64x64 numpy array)\n",
    "    \"\"\"\n",
    "    # CNN √∂zellikleri\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat = cnn_model(image_tensor.unsqueeze(0).to(device))\n",
    "        cnn_feat = feat.squeeze(0).cpu().numpy()  # Boyut: [256]\n",
    "\n",
    "    hog_feat = extract_hog_features(original_image)  # Boyut: √∂rneƒüin ~1764\n",
    "    lpg_feat = extract_lpg_features(original_image)  # Boyut: [36]\n",
    "\n",
    "    # Hibrit √∂zellik vekt√∂r√º: CNN + HOG + LPG\n",
    "    hybrid_feat = np.concatenate([cnn_feat, hog_feat, lpg_feat])\n",
    "    return hybrid_feat\n",
    "\n",
    "# √ñrnek test\n",
    "dummy_tensor = torch.randn(1, 64, 64)  # rastgele √∂rnek (1x64x64)\n",
    "hybrid_test = extract_hybrid_features(dummy_tensor, DeepCNNFeatureExtractor().to(device), test_image)\n",
    "print(\"Hibrit √∂zellik vekt√∂r√º boyutu:\", hybrid_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae4205-e5ec-4f86-aac1-d8b88922f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Dataset Sƒ±nƒ±fƒ± ve Transform Ayarlarƒ±\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class EmotionDatasetFromSamples(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        samples: [(img_path, label)] formatƒ±nda √∂rneklerin listesi\n",
    "        transform: Resim i√ßin uygulanacak d√∂n√º≈ü√ºmler\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        # Orijinal g√∂r√ºnt√ºy√º cv2 ile oku ve yeniden boyutlandƒ±r\n",
    "        original_image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original_image is None:\n",
    "            raise ValueError(f\"Resim okunamadƒ±: {img_path}\")\n",
    "        original_image = cv2.resize(original_image, (64, 64))\n",
    "        # PIL ile a√ßƒ±p grayscale'e √ßevir\n",
    "        image_pil = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)\n",
    "        else:\n",
    "            from torchvision.transforms import ToTensor\n",
    "            image_tensor = ToTensor()(image_pil)\n",
    "        return image_tensor, original_image, label\n",
    "        \n",
    "# Transform: Grayscale, Resize (64x64), ToTensor, Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a02b89-b424-40ba-aaf5-fec22c194d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Hibrit √ñzellik √áƒ±karƒ±m Fonksiyonu ve Veri Yollarƒ±nƒ±n Ayarlanmasƒ±\n",
    "def extract_hybrid_features_from_dataset(dataset, cnn_model):\n",
    "    features_list, labels_list = [], []\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (img_tensor, original_image, label) in enumerate(dataset):\n",
    "            feat = extract_hybrid_features(img_tensor, cnn_model, original_image)\n",
    "            features_list.append(feat)\n",
    "            labels_list.append(label)\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"{idx} √∂rnek i≈ülendi...\")\n",
    "    return np.array(features_list), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9fbf9-9abb-406c-9464-dc172a9e60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Mevcut klas√∂rler\n",
    "base_dir = \"C:/Users/Tuna/Downloads/images\"\n",
    "existing_splits = [\"train\", \"validation\", \"test\"]\n",
    "temp_pool_dir = \"C:/Users/Tuna/Downloads/images_all_temp\"  # ge√ßici havuz\n",
    "\n",
    "# Ge√ßici havuz klas√∂r√ºn√º olu≈ütur\n",
    "os.makedirs(temp_pool_dir, exist_ok=True)\n",
    "\n",
    "# T√ºm verileri ge√ßici havuza kopyala (var olan klas√∂r yapƒ±sƒ± korunmadan)\n",
    "for split in existing_splits:\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "    classes = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(split_dir, cls)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # Ge√ßici havuzda her sƒ±nƒ±f i√ßin klas√∂r olu≈ütur\n",
    "        temp_class_dir = os.path.join(temp_pool_dir, cls)\n",
    "        os.makedirs(temp_class_dir, exist_ok=True)\n",
    "        for img in images:\n",
    "            src = os.path.join(class_path, img)\n",
    "            dst = os.path.join(temp_class_dir, img)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(\"T√ºm veriler ge√ßici havuza kopyalandƒ±:\", temp_pool_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcef63d-8571-4138-8562-820614504473",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"C:\\\\Users\\\\Tuna\\\\Downloads\\\\images_all_temp\"  \n",
    "all_samples = []\n",
    "\n",
    "classes = sorted(os.listdir(train_folder))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classes = sorted(os.listdir(train_folder))\n",
    "\n",
    "for label, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(train_folder, class_name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            all_samples.append((img_path, label))\n",
    "        else:\n",
    "            print(\"Eksik dosya:\", img_path)\n",
    "            \n",
    "# Eƒüitim ve test verilerini ayƒ±r (60% eƒüitim, 40% test)\n",
    "train_samples, test_samples = train_test_split(\n",
    "    all_samples, test_size=0.4, stratify=[label for _, label in all_samples], random_state=42\n",
    ")\n",
    "\n",
    "# Doƒürulama ve test verilerini ayƒ±r (20% + 20%)\n",
    "val_samples, test_samples = train_test_split(\n",
    "    test_samples, test_size=0.5, stratify=[label for _, label in test_samples], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Yeni datasetleri olu≈ütur\n",
    "train_dataset = EmotionDatasetFromSamples(train_samples, transform=transform)\n",
    "val_dataset = EmotionDatasetFromSamples(val_samples, transform=transform)\n",
    "test_dataset = EmotionDatasetFromSamples(test_samples, transform=transform)\n",
    "\n",
    "\n",
    "# Dataloaders olu≈ütur\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Sƒ±nƒ±flar:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad656b2a-eee2-4360-a628-80045b39fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ñznitelik √áƒ±karƒ±mƒ±\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Parametre aralƒ±ƒüƒ±nƒ± daraltƒ±lmƒ±≈ü ve n_iter belirlenmi≈ü parametre daƒüƒ±lƒ±mƒ±\n",
    "param_distributions = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# SVM i√ßin temel fonksiyon\n",
    "def base_svm(**kwargs):\n",
    "    kwargs.setdefault('class_weight', 'balanced')\n",
    "    return SVC(kernel=\"rbf\", **kwargs)\n",
    "\n",
    "# Hibrit √∂zellik √ßƒ±karƒ±mƒ±\n",
    "print(\"Hibrit √∂znitelikler √ßƒ±karƒ±lƒ±yor (CNN + HOG + LPG)...\")\n",
    "\n",
    "# Burada, t√ºm verinin yeniden b√∂l√ºnm√º≈ü datasetlerini (train, validation, test) kullanƒ±yoruz.\n",
    "# Bu datasetlerin, t√ºm veriyi bir havuzda toplayƒ±p 60/20/20 oranƒ±nda b√∂ld√ºƒü√ºn√ºz b√∂l√ºmde olu≈üturduƒüunuzu varsayƒ±yoruz.\n",
    "X_train, y_train = extract_hybrid_features_from_dataset(train_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "X_val, y_val     = extract_hybrid_features_from_dataset(val_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "X_test, y_test   = extract_hybrid_features_from_dataset(test_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "\n",
    "print(\"√ñznitelik √ßƒ±karƒ±mƒ± tamamlandƒ±.\")\n",
    "print(\"Train veri boyutu: \", X_train.shape)\n",
    "print(\"Validation veri boyutu: \", X_val.shape)\n",
    "print(\"Test veri boyutu: \", X_test.shape)\n",
    "\n",
    "# PCA ile boyut indirgeme: √ñrneƒüin, orijinal √∂zellik boyutunu 500'e indiriyoruz\n",
    "pca = PCA(n_components=500, random_state=42)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_val_reduced   = pca.transform(X_val)\n",
    "X_test_reduced  = pca.transform(X_test)\n",
    "\n",
    "print(\"PCA sonrasƒ± boyutlar:\")\n",
    "print(\"Train: \", X_train_reduced.shape)\n",
    "print(\"Validation: \", X_val_reduced.shape)\n",
    "print(\"Test: \", X_test_reduced.shape)\n",
    "\n",
    "# PCA nesnesini kaydedelim\n",
    "joblib.dump(pca, \"pca_transformer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2ab06-acae-4fa6-bc67-e3a7fc9e7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: SVM(RBF) kullanarak eƒüitim\n",
    "\n",
    "# RandomizedSearchCV ile SVM eƒüitimi\n",
    "svm_estimator = base_svm()\n",
    "random_search = RandomizedSearchCV(svm_estimator, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=5, \n",
    "                                   cv=3, \n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=2, \n",
    "                                   random_state=42)\n",
    "\n",
    "def train_random_search():\n",
    "    random_search.fit(X_train_reduced, y_train)\n",
    "\n",
    "print(\"RandomizedSearchCV eƒüitimi ba≈ülƒ±yor (maksimum 2 saat)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    func_timeout(600, train_random_search)  # 7200 saniye = 2 saat\n",
    "except FunctionTimedOut:\n",
    "    print(\"Eƒüitim 2 saat sonra zaman a≈üƒ±mƒ±na uƒüradƒ±. O ana kadarki en iyi model kullanƒ±lacak.\")\n",
    "except Exception as e:\n",
    "    print(\"Eƒüitim sƒ±rasƒ±nda beklenmeyen hata olu≈ütu:\", e)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Eƒüitim s√ºresi: {elapsed/60:.2f} dakika\")\n",
    "\n",
    "# O ana kadar en iyi modeli belirleme\n",
    "best_model = None\n",
    "if hasattr(random_search, 'best_estimator_') and random_search.best_estimator_ is not None:\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(\"En iyi parametreler:\", random_search.best_params_)\n",
    "else:\n",
    "    print(\"RandomizedSearchCV tamamlanmadƒ± veya best_estimator_ olu≈ümadƒ±, en iyi parametreler belirlenecek.\")\n",
    "    if hasattr(random_search, 'cv_results_'):\n",
    "        best_index = np.argmax(random_search.cv_results_['mean_test_score'])\n",
    "        best_params = random_search.cv_results_['params'][best_index]\n",
    "        print(\"En iyi parametreler (ge√ßici olarak):\", best_params)\n",
    "        best_model = SVC(kernel=\"rbf\", class_weight=\"balanced\", **best_params)\n",
    "        best_model.fit(X_train_reduced, y_train)\n",
    "    else:\n",
    "        print(\"cv_results_ yok, model olu≈üturulamadƒ±.\")\n",
    "\n",
    "# Modeli kaydet\n",
    "if best_model:\n",
    "    model_path = \"best_svm_model.pkl\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Model kaydedildi: {model_path}\")\n",
    "else:\n",
    "    print(\"Hata: Model kaydedilemedi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708f83e-6d60-4822-9d73-f0d73145445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En iyi model ile test tahmini yapma\n",
    "if best_model:\n",
    "    print(\"\\nTest seti √ºzerinde tahmin yapƒ±lƒ±yor...\")\n",
    "    y_pred = best_model.predict(X_test_reduced)\n",
    "    print(\"Tahmin tamamlandƒ±.\\n\")\n",
    "    print(\"Sƒ±nƒ±flandƒ±rma Raporu:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=classes))\n",
    "else:\n",
    "    print(\"Hata: RandomizedSearch tamamlanmadƒ±ƒüƒ± i√ßin tahmin yapƒ±lamadƒ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526d6a8-828e-47e7-8893-6338a617b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Model Kaydetme, Y√ºkleme ve Test G√∂rselle≈ütirme (G√ºncellenmi≈ü)\n",
    "import random\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kaydedilen SVM modelini y√ºkleyelim\n",
    "best_model = joblib.load(\"best_svm_model.pkl\")\n",
    "# Kaydedilen PCA nesnesini de y√ºkleyelim\n",
    "pca = joblib.load(\"pca_transformer.pkl\")\n",
    "\n",
    "# Aynƒ± √∂zellik √ßƒ±karƒ±mƒ±nda kullandƒ±ƒüƒ±nƒ±z CNN modelini yeniden olu≈üturun\n",
    "cnn_extractor = DeepCNNFeatureExtractor().to(device)\n",
    "cnn_extractor.eval()\n",
    "\n",
    "# Validation dataset'inden rastgele bir √∂rnek se√ßelim\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "img_tensor, original_image, true_label = val_dataset[idx]\n",
    "\n",
    "# Hibrit √∂zellikleri √ßƒ±karalƒ±m (CNN, HOG, LPG)\n",
    "hybrid_feat = extract_hybrid_features(img_tensor, cnn_extractor, original_image)  # Boyut: 2056\n",
    "# PCA d√∂n√º≈ü√ºm√ºn√º uygulayalƒ±m: 2056 -> 500\n",
    "X_sample = pca.transform(hybrid_feat.reshape(1, -1))\n",
    "\n",
    "# SVM modeli ile tahmin yapalƒ±m\n",
    "predicted_label = best_model.predict(X_sample)[0]\n",
    "\n",
    "# Sonu√ßlarƒ± g√∂rselle≈ütirme\n",
    "plt.imshow(original_image, cmap=\"gray\")\n",
    "plt.title(f\"Ger√ßek Etiket: {classes[true_label]}\\nTahmin: {classes[predicted_label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231617a-9f0e-42af-8ba0-35dded69593b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
