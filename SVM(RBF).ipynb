{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caf21b-5624-4ee7-ae0a-ad18ae53ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Gerekli Kütüphaneler ve Ayarlar\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Cihaz ayarı (GPU varsa GPU, yoksa CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431d3f8-d82a-439d-a782-bf665d01cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Kök klasör\n",
    "base_dir = \"C:/Users/Tuna/Downloads/images\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Sınıflar (örneğin: happy, sad, etc.)\n",
    "classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "\n",
    "# Test klasörünü oluştur\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"\\n▶️ {cls} sınıfı için örnekler hazırlanıyor...\")\n",
    "\n",
    "    # Tüm resimleri topla\n",
    "    train_cls_path = os.path.join(train_dir, cls)\n",
    "    val_cls_path = os.path.join(val_dir, cls)\n",
    "\n",
    "    all_images = []\n",
    "\n",
    "    for folder in [train_cls_path, val_cls_path]:\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        all_images += [(os.path.join(folder, f), f) for f in files]\n",
    "\n",
    "    # Karıştır\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    total_count = len(all_images)\n",
    "    test_count = int(0.2 * total_count)\n",
    "\n",
    "    print(f\"Toplam: {total_count} -> Test'e ayrılacak: {test_count} görsel\")\n",
    "\n",
    "    # Test için seçilen görselleri yeni klasöre kopyala\n",
    "    selected_test_images = all_images[:test_count]\n",
    "    for src_path, filename in selected_test_images:\n",
    "        dst_path = os.path.join(test_dir, cls, filename)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"✅ {cls}: Test seti tamamlandı. ({len(selected_test_images)} resim)\")\n",
    "\n",
    "print(\"\\n🎉 Tüm sınıflar için test seti başarıyla oluşturuldu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706eda0-7c6f-46c1-9dee-8a8f8e6b41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: CNN Modeli Tanımlaması (DeepCNNFeatureExtractor)\n",
    "# Bu model, 1 kanallı (grayscale) 64x64 görüntüler için sıfırdan eğitilecek şekilde tasarlanmıştır.\n",
    "class DeepCNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNNFeatureExtractor, self).__init__()\n",
    "        # Blok 1: 1 -> 32 kanal\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 64x64 -> 32x32\n",
    "\n",
    "        # Blok 2: 32 -> 64 kanal\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
    "\n",
    "        # Blok 3: 64 -> 128 kanal\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5   = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6   = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 16x16 -> 8x8\n",
    "\n",
    "        # Tam bağlantılı katman: Özellik vektörü boyutu 128*8*8 = 8192'den 256'ya indirme\n",
    "        self.fc = nn.Linear(128 * 8 * 8, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.fc(x)  # 256 boyutlu özellik vektörü\n",
    "        return features\n",
    "\n",
    "# Örnek test (isteğe bağlı)\n",
    "if __name__ == '__main__':\n",
    "    dummy = torch.randn(1, 1, 64, 64).to(device)\n",
    "    feat = DeepCNNFeatureExtractor().to(device)(dummy)\n",
    "    print(\"CNN özellik vektörü boyutu:\", feat.shape)  # Beklenen: [1, 256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2c52a-0b35-407c-a5e7-52ec2e5b9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: HOG Özellik Çıkarım Fonksiyonu\n",
    "def extract_hog_features(image, pixels_per_cell=(8,8), cells_per_block=(2,2), orientations=9):\n",
    "    \"\"\"\n",
    "    image: Giriş görüntüsü (grayscale, numpy array, boyut: 64x64)\n",
    "    HOG özellikleri çıkarılır.\n",
    "    \"\"\"\n",
    "    # image'ı float formatına çeviriyoruz\n",
    "    image = np.float32(image) / 255.0\n",
    "    features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block, block_norm='L2-Hys', feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Örnek test\n",
    "test_image = np.random.randint(0, 256, (64, 64), dtype=np.uint8)\n",
    "hog_feat = extract_hog_features(test_image)\n",
    "print(\"HOG özelliği boyutu:\", hog_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87699a30-87bd-4a93-8309-4d046c199a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: LPG Özellik Çıkarım Fonksiyonu (Sobel Gradyanları ile)\n",
    "def extract_lpg_features(image):\n",
    "    \"\"\"\n",
    "    image: Giriş görüntüsü (grayscale, numpy array, boyut: 64x64)\n",
    "    Sobel gradyanları kullanılarak yön histogramı hesaplanır.\n",
    "    \"\"\"\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    angle = cv2.phase(grad_x, grad_y, angleInDegrees=True)\n",
    "    hist = cv2.calcHist([np.uint8(angle)], [0], None, [36], [0, 360])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# Örnek test\n",
    "lpg_feat = extract_lpg_features(test_image)\n",
    "print(\"LPG özelliği boyutu:\", lpg_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda70f5c-7fd9-4b5d-bfb4-63a906d40270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Hibrit Özellik Çıkarım Fonksiyonu (CNN, HOG, LPG)\n",
    "def extract_hybrid_features(image_tensor, cnn_model, original_image):\n",
    "    \"\"\"\n",
    "    image_tensor: CNN için ön işlenmiş tensör (1x64x64)\n",
    "    original_image: Orijinal veya yeniden boyutlandırılmış grayscale görüntü (64x64 numpy array)\n",
    "    \"\"\"\n",
    "    # CNN özellikleri\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat = cnn_model(image_tensor.unsqueeze(0).to(device))\n",
    "        cnn_feat = feat.squeeze(0).cpu().numpy()  # Boyut: [256]\n",
    "\n",
    "    hog_feat = extract_hog_features(original_image)  # Boyut: örneğin ~1764\n",
    "    lpg_feat = extract_lpg_features(original_image)  # Boyut: [36]\n",
    "\n",
    "    # Hibrit özellik vektörü: CNN + HOG + LPG\n",
    "    hybrid_feat = np.concatenate([cnn_feat, hog_feat, lpg_feat])\n",
    "    return hybrid_feat\n",
    "\n",
    "# Örnek test\n",
    "dummy_tensor = torch.randn(1, 64, 64)  # rastgele örnek (1x64x64)\n",
    "hybrid_test = extract_hybrid_features(dummy_tensor, DeepCNNFeatureExtractor().to(device), test_image)\n",
    "print(\"Hibrit özellik vektörü boyutu:\", hybrid_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae4205-e5ec-4f86-aac1-d8b88922f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Dataset Sınıfı ve Transform Ayarları\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class EmotionDatasetFromSamples(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        samples: [(img_path, label)] formatında örneklerin listesi\n",
    "        transform: Resim için uygulanacak dönüşümler\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        # Orijinal görüntüyü cv2 ile oku ve yeniden boyutlandır\n",
    "        original_image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original_image is None:\n",
    "            raise ValueError(f\"Resim okunamadı: {img_path}\")\n",
    "        original_image = cv2.resize(original_image, (64, 64))\n",
    "        # PIL ile açıp grayscale'e çevir\n",
    "        image_pil = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)\n",
    "        else:\n",
    "            from torchvision.transforms import ToTensor\n",
    "            image_tensor = ToTensor()(image_pil)\n",
    "        return image_tensor, original_image, label\n",
    "        \n",
    "# Transform: Grayscale, Resize (64x64), ToTensor, Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a02b89-b424-40ba-aaf5-fec22c194d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Hibrit Özellik Çıkarım Fonksiyonu ve Veri Yollarının Ayarlanması\n",
    "def extract_hybrid_features_from_dataset(dataset, cnn_model):\n",
    "    features_list, labels_list = [], []\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (img_tensor, original_image, label) in enumerate(dataset):\n",
    "            feat = extract_hybrid_features(img_tensor, cnn_model, original_image)\n",
    "            features_list.append(feat)\n",
    "            labels_list.append(label)\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"{idx} örnek işlendi...\")\n",
    "    return np.array(features_list), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9fbf9-9abb-406c-9464-dc172a9e60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Mevcut klasörler\n",
    "base_dir = \"C:/Users/Tuna/Downloads/images\"\n",
    "existing_splits = [\"train\", \"validation\", \"test\"]\n",
    "temp_pool_dir = \"C:/Users/Tuna/Downloads/images_all_temp\"  # geçici havuz\n",
    "\n",
    "# Geçici havuz klasörünü oluştur\n",
    "os.makedirs(temp_pool_dir, exist_ok=True)\n",
    "\n",
    "# Tüm verileri geçici havuza kopyala (var olan klasör yapısı korunmadan)\n",
    "for split in existing_splits:\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "    classes = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(split_dir, cls)\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # Geçici havuzda her sınıf için klasör oluştur\n",
    "        temp_class_dir = os.path.join(temp_pool_dir, cls)\n",
    "        os.makedirs(temp_class_dir, exist_ok=True)\n",
    "        for img in images:\n",
    "            src = os.path.join(class_path, img)\n",
    "            dst = os.path.join(temp_class_dir, img)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(\"Tüm veriler geçici havuza kopyalandı:\", temp_pool_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcef63d-8571-4138-8562-820614504473",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"C:\\\\Users\\\\Tuna\\\\Downloads\\\\images_all_temp\"  \n",
    "all_samples = []\n",
    "\n",
    "classes = sorted(os.listdir(train_folder))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classes = sorted(os.listdir(train_folder))\n",
    "\n",
    "for label, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(train_folder, class_name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            all_samples.append((img_path, label))\n",
    "        else:\n",
    "            print(\"Eksik dosya:\", img_path)\n",
    "            \n",
    "# Eğitim ve test verilerini ayır (60% eğitim, 40% test)\n",
    "train_samples, test_samples = train_test_split(\n",
    "    all_samples, test_size=0.4, stratify=[label for _, label in all_samples], random_state=42\n",
    ")\n",
    "\n",
    "# Doğrulama ve test verilerini ayır (20% + 20%)\n",
    "val_samples, test_samples = train_test_split(\n",
    "    test_samples, test_size=0.5, stratify=[label for _, label in test_samples], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Yeni datasetleri oluştur\n",
    "train_dataset = EmotionDatasetFromSamples(train_samples, transform=transform)\n",
    "val_dataset = EmotionDatasetFromSamples(val_samples, transform=transform)\n",
    "test_dataset = EmotionDatasetFromSamples(test_samples, transform=transform)\n",
    "\n",
    "\n",
    "# Dataloaders oluştur\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Sınıflar:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad656b2a-eee2-4360-a628-80045b39fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Öznitelik Çıkarımı\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Parametre aralığını daraltılmış ve n_iter belirlenmiş parametre dağılımı\n",
    "param_distributions = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# SVM için temel fonksiyon\n",
    "def base_svm(**kwargs):\n",
    "    kwargs.setdefault('class_weight', 'balanced')\n",
    "    return SVC(kernel=\"rbf\", **kwargs)\n",
    "\n",
    "# Hibrit özellik çıkarımı\n",
    "print(\"Hibrit öznitelikler çıkarılıyor (CNN + HOG + LPG)...\")\n",
    "\n",
    "# Burada, tüm verinin yeniden bölünmüş datasetlerini (train, validation, test) kullanıyoruz.\n",
    "# Bu datasetlerin, tüm veriyi bir havuzda toplayıp 60/20/20 oranında böldüğünüz bölümde oluşturduğunuzu varsayıyoruz.\n",
    "X_train, y_train = extract_hybrid_features_from_dataset(train_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "X_val, y_val     = extract_hybrid_features_from_dataset(val_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "X_test, y_test   = extract_hybrid_features_from_dataset(test_dataset, DeepCNNFeatureExtractor().to(device))\n",
    "\n",
    "print(\"Öznitelik çıkarımı tamamlandı.\")\n",
    "print(\"Train veri boyutu: \", X_train.shape)\n",
    "print(\"Validation veri boyutu: \", X_val.shape)\n",
    "print(\"Test veri boyutu: \", X_test.shape)\n",
    "\n",
    "# PCA ile boyut indirgeme: Örneğin, orijinal özellik boyutunu 500'e indiriyoruz\n",
    "pca = PCA(n_components=500, random_state=42)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_val_reduced   = pca.transform(X_val)\n",
    "X_test_reduced  = pca.transform(X_test)\n",
    "\n",
    "print(\"PCA sonrası boyutlar:\")\n",
    "print(\"Train: \", X_train_reduced.shape)\n",
    "print(\"Validation: \", X_val_reduced.shape)\n",
    "print(\"Test: \", X_test_reduced.shape)\n",
    "\n",
    "# PCA nesnesini kaydedelim\n",
    "joblib.dump(pca, \"pca_transformer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2ab06-acae-4fa6-bc67-e3a7fc9e7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: SVM(RBF) kullanarak eğitim\n",
    "\n",
    "# RandomizedSearchCV ile SVM eğitimi\n",
    "svm_estimator = base_svm()\n",
    "random_search = RandomizedSearchCV(svm_estimator, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=5, \n",
    "                                   cv=3, \n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=2, \n",
    "                                   random_state=42)\n",
    "\n",
    "def train_random_search():\n",
    "    random_search.fit(X_train_reduced, y_train)\n",
    "\n",
    "print(\"RandomizedSearchCV eğitimi başlıyor (maksimum 2 saat)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    func_timeout(600, train_random_search)  # 7200 saniye = 2 saat\n",
    "except FunctionTimedOut:\n",
    "    print(\"Eğitim 2 saat sonra zaman aşımına uğradı. O ana kadarki en iyi model kullanılacak.\")\n",
    "except Exception as e:\n",
    "    print(\"Eğitim sırasında beklenmeyen hata oluştu:\", e)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Eğitim süresi: {elapsed/60:.2f} dakika\")\n",
    "\n",
    "# O ana kadar en iyi modeli belirleme\n",
    "best_model = None\n",
    "if hasattr(random_search, 'best_estimator_') and random_search.best_estimator_ is not None:\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(\"En iyi parametreler:\", random_search.best_params_)\n",
    "else:\n",
    "    print(\"RandomizedSearchCV tamamlanmadı veya best_estimator_ oluşmadı, en iyi parametreler belirlenecek.\")\n",
    "    if hasattr(random_search, 'cv_results_'):\n",
    "        best_index = np.argmax(random_search.cv_results_['mean_test_score'])\n",
    "        best_params = random_search.cv_results_['params'][best_index]\n",
    "        print(\"En iyi parametreler (geçici olarak):\", best_params)\n",
    "        best_model = SVC(kernel=\"rbf\", class_weight=\"balanced\", **best_params)\n",
    "        best_model.fit(X_train_reduced, y_train)\n",
    "    else:\n",
    "        print(\"cv_results_ yok, model oluşturulamadı.\")\n",
    "\n",
    "# Modeli kaydet\n",
    "if best_model:\n",
    "    model_path = \"best_svm_model.pkl\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Model kaydedildi: {model_path}\")\n",
    "else:\n",
    "    print(\"Hata: Model kaydedilemedi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708f83e-6d60-4822-9d73-f0d73145445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En iyi model ile test tahmini yapma\n",
    "if best_model:\n",
    "    print(\"\\nTest seti üzerinde tahmin yapılıyor...\")\n",
    "    y_pred = best_model.predict(X_test_reduced)\n",
    "    print(\"Tahmin tamamlandı.\\n\")\n",
    "    print(\"Sınıflandırma Raporu:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=classes))\n",
    "else:\n",
    "    print(\"Hata: RandomizedSearch tamamlanmadığı için tahmin yapılamadı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526d6a8-828e-47e7-8893-6338a617b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Model Kaydetme, Yükleme ve Test Görselleştirme (Güncellenmiş)\n",
    "import random\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kaydedilen SVM modelini yükleyelim\n",
    "best_model = joblib.load(\"best_svm_model.pkl\")\n",
    "# Kaydedilen PCA nesnesini de yükleyelim\n",
    "pca = joblib.load(\"pca_transformer.pkl\")\n",
    "\n",
    "# Aynı özellik çıkarımında kullandığınız CNN modelini yeniden oluşturun\n",
    "cnn_extractor = DeepCNNFeatureExtractor().to(device)\n",
    "cnn_extractor.eval()\n",
    "\n",
    "# Validation dataset'inden rastgele bir örnek seçelim\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "img_tensor, original_image, true_label = val_dataset[idx]\n",
    "\n",
    "# Hibrit özellikleri çıkaralım (CNN, HOG, LPG)\n",
    "hybrid_feat = extract_hybrid_features(img_tensor, cnn_extractor, original_image)  # Boyut: 2056\n",
    "# PCA dönüşümünü uygulayalım: 2056 -> 500\n",
    "X_sample = pca.transform(hybrid_feat.reshape(1, -1))\n",
    "\n",
    "# SVM modeli ile tahmin yapalım\n",
    "predicted_label = best_model.predict(X_sample)[0]\n",
    "\n",
    "# Sonuçları görselleştirme\n",
    "plt.imshow(original_image, cmap=\"gray\")\n",
    "plt.title(f\"Gerçek Etiket: {classes[true_label]}\\nTahmin: {classes[predicted_label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231617a-9f0e-42af-8ba0-35dded69593b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
